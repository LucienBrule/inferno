# ğŸ–¥ï¸ Compute Architecture

This document describes the compute layer design for **Project Inferno** in *Avernus* (1544 Bel Aire Dr). It covers hardware profiles, software orchestration, naming, and scheduling policies. All names follow the Inferno taxonomy.

---

## Node Classes & Rack Assignment

### Circle 2: Lust (GPU Compute)
- **Rack ID**: `circle-2-lust`
- **Hosts**:
  - `inferno-host-compute-1` (primary)
- **Hardware Profile**:
  - **Chassis**: Supermicro AS-4125GS-TNR (4U, 8Ã— GPU)
  - **CPU**: 2Ã— AMD EPYC 9655 (96â€‘core, 2.6â€¯GHz, 400â€¯W)
  - **Memory**: 24Ã— 32â€¯GB DDR5â€‘6400 ECC RDIMM (=768â€¯GB, 1:4 core:GB)
  - **GPUs**: 4Ã— NVIDIA RTX PROâ€¯6000 Blackwell (96â€¯GB, 300â€¯W)
  - **Storage**:
    - 4Ã— 3.84â€¯TB PCIeâ€¯5.0 NVMe (scratch/model store)
    - 2Ã— 960â€¯GB SATA SSD (boot, mirrored)
- **Power**: ~2.3â€¯kW draw, fed from NEMAâ€¯6â€‘30R â†’ `inferno-pdu-circle2-1`
- **Use Cases**: Highâ€‘density inference, model fineâ€‘tuning, batch training.

### Circle 6: Heresy (Experimental & Alchemy)
- **Rack ID**: `circle-6-heresy`
- **Hosts**:
  - `inferno-host-exper-1`
- **Hardware Profile**:
  - **Chassis**: Supermicro AS-4125GS-TNR (4U)
  - **CPU**: 1Ã— AMD EPYC 9334 (32â€‘core, 2.7â€¯GHz, 210â€¯W)
  - **Memory**: 16Ã— 32â€¯GB DDR5â€‘6400 ECC RDIMM (=512â€¯GB)
  - **GPUs**: 2Ã— NVIDIA L40S (48â€¯GB, 350â€¯W)
  - **Storage**: 2Ã— 3.84â€¯TB NVMe scratch
- **Power**: ~1.1â€¯kW, fed from NEMAâ€¯6â€‘20R â†’ `inferno-pdu-circle6-1`
- **Use Cases**: Prototype architectures, sideâ€‘byâ€‘side benchmarks, sandbox ML experiments.

### Circle 1: Limbo (Computeâ€‘Light & Orchestration)
- **Rack ID**: `circle-1-limbo`
- **Hosts**:
  - `inferno-host-master`
- **Hardware Profile**:
  - **Chassis**: Supermicro EATX server (2U)
  - **CPU**: 2Ã— AMD EPYC 9354 (32â€‘core, 3.25â€¯GHz, 280â€¯W)
  - **Memory**: 8Ã— 32â€¯GB DDR5â€‘6400 ECC (=256â€¯GB)
  - **Storage**: 2Ã— 960â€¯GB SSD (OS & control plane)
- **Power**: ~600â€¯W, fed from C20 â†’ `inferno-pdu-circle1-1`
- **Use Cases**: OpenShift masters, DNS, registry, CI/CD runners.

---

## Node Pool & Scheduling

- **Kubernetes Node Pools**:
  - `inferno-pool-gpu` â†’ label `inferno.circle=2`
  - `inferno-pool-exper` â†’ label `inferno.circle=6`
  - `inferno-pool-master` â†’ label `inferno.circle=1`
- **Taints & Tolerations**:
  - GPU nodes: `inferno/gpu=true:NoSchedule`
  - Experimental: `inferno/experiment=true:PreferNoSchedule`
- **Resource Reservations**:
  - Reserve 10% CPU & 1â€¯GB RAM for node daemons.
  - GPU nodes expose `nvidia.com/gpu` resources.

---

## Networking & Fabric

- **Host Interfaces**:
  - `eth0`: Management network (10â€¯GbE)
  - `eth1`: Data plane (25â€¯GbE) â†’ connected to `inferno-switch-aggregation-1`
  - `ib0` (optional): RoCEv2 for NVMeâ€‘oF
- **CNI**: Calico with IPâ€‘perâ€‘Pod.  
- **Ingress/Egress**: Egress via DPU (BlueField-3) for offloaded NVMeâ€‘oF or iSCSI.

---

## Software Stack

- **OS**: Rockyâ€¯Linux 9.3, tuned for HPC.
- **Container Runtime**: CRIâ€‘O
- **Kubernetes**: OpenShift 4.14
- **GPU Operator**: NVIDIA GPU Operator for driver & runtime.
- **Storage CSI**:  
  - ZFSâ€¯LocalPV for NVMe scratch volumes.  
  - Rookâ€¯Ceph for warm tier connectivity (optional).
- **Monitoring & Logging**:
  - Prometheus + Node Exporter  
  - ELK stack on Circleâ€¯1  
  - SNMP via PDU and BMC

---

## Daemon Services & Namespaces

- **Namespaces**:
  - `inferno-circle-2` (GPU workloads)
  - `inferno-circle-6` (experimental)
  - `inferno-circle-1` (platform services)
- **DaemonSets/Deployments**:
  - `daemon-gpu-watcher`  
  - `daemon-storage-sync`  
  - `daemon-hvac-monitor`  
  - `daemon-power-monitor`

---

*All components, node labels, and resource definitions must adhere to the Inferno taxonomy and be reviewed under Phaseâ€¯1 planning.*